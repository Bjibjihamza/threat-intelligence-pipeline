import os
import time
import csv
import requests
from requests.exceptions import RequestException, HTTPError
from datetime import datetime, date

API_KEY = os.getenv("CVEFEED_API_KEY", "a365c3a8370c3e057d107bdc235776d5ea77293d")
BASE = "https://cvefeed.io"
HEADERS = {"Authorization": f"Token {API_KEY}"} if API_KEY else {}

# Config
START_PAGE = 1
MAX_PAGES = None        # set to an int to limit pages (e.g., 100). None => no explicit page cap.
PER_REQUEST_DELAY = 0.5 # seconds between requests (polite). Increase if you see 429s.
OUT_PREFIX = "vulnerabilities"  # output file prefix

FIELDS = [
    "id", "title", "published", "last_modified",
    "cvss_score", "severity", "cvss_version",
    "url", "source_identifier", "status"
]

today = date.today()

def parse_published(pub_str):
    if not pub_str:
        return None
    # Handle ISO format with Z
    pub_str = pub_str.replace('Z', '+00:00')
    return datetime.fromisoformat(pub_str)

def fetch_page(page, max_retries=6):
    url = f"{BASE}/api/vulnerability/"
    params = {"page": page}
    attempt = 0
    backoff = 1
    while True:
        try:
            r = requests.get(url, headers=HEADERS, params=params, timeout=30)
            # Raise for non-2xx
            r.raise_for_status()
            return r.json()
        except HTTPError as e:
            status = getattr(e.response, "status_code", None)
            # Handle rate limiting specially
            if status == 429:
                retry_after = e.response.headers.get("Retry-After")
                if retry_after:
                    wait = float(retry_after)
                else:
                    wait = backoff
                    backoff = min(backoff * 2, 60)
                print(f"[429] Rate limited. Waiting {wait:.1f}s before retrying (page {page})...")
                time.sleep(wait)
                attempt += 1
            else:
                attempt += 1
                if attempt > max_retries:
                    raise
                print(f"[HTTPError {status}] retrying page {page} in {backoff}s (attempt {attempt})...")
                time.sleep(backoff)
                backoff = min(backoff * 2, 60)
        except RequestException as e:
            attempt += 1
            if attempt > max_retries:
                raise
            print(f"[RequestException] {e}; retrying page {page} in {backoff}s (attempt {attempt})...")
            time.sleep(backoff)
            backoff = min(backoff * 2, 60)

def save_csv(rows, total_count):
    outfile = f"{OUT_PREFIX}_{total_count}.csv"
    with open(outfile, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=FIELDS)
        writer.writeheader()
        for r in rows:
            writer.writerow({k: r.get(k, "") for k in FIELDS})
    print(f"✅ Saved {len(rows)} rows to {outfile}")

def main():
    page = START_PAGE
    total = 0
    seen_ids = set()
    cumulative_rows = []
    batch_rows = []

    while True:
        if MAX_PAGES is not None and page - START_PAGE + 1 > MAX_PAGES:
            print(f"Reached MAX_PAGES ({MAX_PAGES}). Stopping.")
            break

        print(f"Fetching page {page} ...")
        data = fetch_page(page)
        results = data.get("results", []) or []

        if not results:
            print("No results returned — finished.")
            break

        # Check the first result's published date
        first_pub_dt = None
        if results:
            first_pub = results[0].get("published")
            first_pub_dt = parse_published(first_pub)
            if first_pub_dt and first_pub_dt.date() < today:
                print(f"First entry on page {page} is from {first_pub_dt.date()} (before today {today}). Stopping.")
                break

        new_count = 0
        stopped_early = False
        for v in results:
            pub = v.get("published")
            pub_dt = parse_published(pub)
            if pub_dt and pub_dt.date() == today:
                vid = v.get("id")
                if not vid or vid in seen_ids:
                    continue
                seen_ids.add(vid)
                row = {k: v.get(k, "") for k in FIELDS}
                batch_rows.append(row)
                total += 1
                new_count += 1
            elif pub_dt and pub_dt.date() < today:
                # Assuming sorted by published descending, stop processing this page early
                print(f"Encountered entry from {pub_dt.date()} (before today). Stopping page {page} early.")
                stopped_early = True
                break

            # Save a file every 100 results
            if len(batch_rows) == 100:
                save_csv(batch_rows, total)
                batch_rows = []  # reset for next 100

        if stopped_early:
            break

        print(f"Page {page}: {new_count} new CVEs from today, cumulative total = {total}")

        next_link = data.get("next")
        if next_link:
            page += 1
        else:
            page += 1
            if not data.get("next") and page > (data.get("count", 0) // 10 + 5):
                print("No 'next' link and we've paged past expected range — stopping.")
                break

        time.sleep(PER_REQUEST_DELAY)

    # Save remaining (if <100 left)
    if batch_rows:
        save_csv(batch_rows, total)

    print("Done.")


if __name__ == "__main__":
    main()